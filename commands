sbatch submit.sh python train.py \
    --config src/vq_vae/configuration/train.ini \
    --model vq-vae \
    --wandb_dir /tmp/dzverev_data/ \
    --max_epochs 220 \
    --max_epochs_igpt 300 \
    --max_epochs_lin_eval 250 \
    --num_random_future_samples 2000 \
    --num_random_past_samples 7000 \
    --downstream_loss_weight 0 \
    --evaluation_logger wandb \
    --train_logger wandb \
    --group igpt-bootstrap \
    --num_workers 12 \
    --future_samples_mode noiseg

# CIFAR 100
TIMM_FUSED_ATTN=1 sbatch submit.sh python train.py \
    --config src/transformer_vq_vae/configuration/train.ini \
    --model transformer-vq-vae \
    --wandb_dir /tmp/dzverev_data/ \
    --group mae-cifar100 \
    --batch_size 512 \
    --accumulate_grad_batches 1 \
    --num_workers 4 \
    --max_epochs 1500 \
    --min_epochs 1500 \
    --max_epochs_lin_eval 100 \
    --num_random_past_samples 30000 \
    --num_random_past_samples_schedule schedule \
    --num_random_future_samples 0 \
    --future_samples_mode noise \
    --cycle_consistency_loss_weight_for_past 1 \
    --cycle_consistency_loss_weight_for_current 0 \
    --cycle_consistency_sigma 200 \
    --past_samples_loss_weight 0 \
    --current_samples_loss_weight 1 \
    --future_samples_loss_weight 1 \
    --data_variance 0.071093 \
    --perplexity_threshold 30 \
    --quantize_top_k 1 \
    --encoder_layer 12 \
    --encoder_head 3 \
    --decoder_layer 4 \
    --decoder_head 3 \
    --use_lpips True \
    --dataset cifar100 \
    --patch_size 2 \
    --num_embeddings 512 \
    --num_embeddings_per_step 64\
    --num_tasks 20 \
    --supervised True \
    --precision 16-mixed

# CIFAR 10
python submitit_train.py \
    --cpus_per_task 8 \
    --slurm_mem 96 \
    --gpu_type a6000 \
    --partition ddp-4way \
    --timeout 2880 \
    --ngpus 1 \
    --comment "vq-mae 5 tasks" \
    --group mae-cifar10 \
    --config src/transformer_vq_vae/configuration/train.ini \
    --model transformer-vq-vae \
    --wandb_dir /tmp/dzverev_data/ \
    --devices 0, \
    --batch_size 256 \
    --accumulate_grad_batches 2 \
    --num_workers 4 \
    --max_epochs 600 \
    --min_epochs 600 \
    --max_epochs_lin_eval 100 \
    --num_random_past_samples 20000 \
    --num_random_past_samples_schedule schedule \
    --num_random_future_samples 0 \
    --future_samples_mode noise \
    --cycle_consistency_loss_weight_for_past 1 \
    --cycle_consistency_loss_weight_for_current 0 \
    --cycle_consistency_sigma 200 \
    --past_samples_loss_weight 1 \
    --current_samples_loss_weight 1 \
    --future_samples_loss_weight 0 \
    --data_variance 0.06328692405746414 \
    --perplexity_threshold 0 \
    --quantize_top_k 1 \
    --encoder_layer 12 \
    --encoder_head 3 \
    --decoder_layer 4 \
    --decoder_head 3 \
    --use_lpips True \
    --dataset cifar10 \
    --patch_size 2 \
    --num_embeddings 512 \
    --num_embeddings_per_step 64\
    --num_tasks 5 \
    --supervised False \
    --precision 16-mixed

# imagenet
TIMM_FUSED_ATTN=1 sbatch submit.sh python train.py \
    --config src/transformer_vq_vae/configuration/train.ini \
    --model transformer-vq-vae \
    --wandb_dir /tmp/dzverev_data/ \
    --group mae-imagenet \
    --devices 0,1 \
    --strategy ddp \
    --batch_size 64 \
    --accumulate_grad_batches 64 \
    --num_workers 16 \
    --max_epochs 400 \
    --min_epochs 400 \
    --max_epochs_lin_eval 100 \
    --num_random_past_samples 20000 \
    --num_random_past_samples_schedule schedule \
    --num_random_future_samples 0 \
    --future_samples_mode noise \
    --cycle_consistency_loss_weight_for_past 1 \
    --cycle_consistency_loss_weight_for_current 0 \
    --cycle_consistency_sigma 200 \
    --past_samples_loss_weight 0 \
    --current_samples_loss_weight 1 \
    --future_samples_loss_weight 1 \
    --data_variance 1 \
    --perplexity_threshold 30 \
    --quantize_top_k 1 \
    --encoder_layer 12 \
    --encoder_head 12 \
    --decoder_layer 8 \
    --decoder_head 8 \
    --embedding_dim 768 \
    --decoder_embedding_dim 768 \
    --use_lpips True \
    --dataset imagenet \
    --image_size 224 \
    --patch_size 16 \
    --learning_rate 0.0001 \
    --weight_decay 0.3 \
    --warmup 20 \
    --num_embeddings 1024 \
    --num_embeddings_per_step 64\
    --num_tasks 5 \
    --supervised False \
    --precision 16-mixed

# Dry run
TIMM_FUSED_ATTN=1 sbatch submit.sh python train.py \
    --config src/transformer_vq_vae/configuration/train.ini \
    --model transformer-vq-vae \
    --wandb_dir /tmp/dzverev_data/ \
    --group mae-cifar100 \
    --batch_size 256 \
    --accumulate_grad_batches 1 \
    --num_workers 4 \
    --max_epochs 2 \
    --min_epochs 2 \
    --max_epochs_lin_eval 2 \
    --num_random_past_samples 100 \
    --num_random_past_samples_schedule fixed \
    --num_random_future_samples 0 \
    --future_samples_mode noise \
    --cycle_consistency_loss_weight_for_past 1 \
    --cycle_consistency_loss_weight_for_current 0 \
    --cycle_consistency_sigma 200 \
    --past_samples_loss_weight 0 \
    --current_samples_loss_weight 1 \
    --future_samples_loss_weight 1 \
    --data_variance 0.071093 \
    --perplexity_threshold 30 \
    --quantize_top_k 1 \
    --encoder_layer 12 \
    --encoder_head 3 \
    --decoder_layer 4 \
    --decoder_head 3 \
    --use_lpips True \
    --dataset cifar100 \
    --patch_size 2 \
    --num_embeddings 512 \
    --num_embeddings_per_step 64\
    --num_tasks 5 \
    --supervised False \
    --igpt_epoch_num 2\
    --precision 16-mixed