sbatch -J cka_scores_2eu0srw6 submit.sh \
    python commands.py \
    --command cka_score \
    --batch_size 128 \
    --max_epochs 1000 \
    --min_epochs 200 \
    --run_id 2eu0srw6

sbatch -J fid_scores_1z0vahmb submit.sh \
    python commands.py \
    --command fid_score \
    --max_epochs 600 \
    --min_epochs 600 \
    --run_id 1z0vahmb

python submitit_train.py \
    --cpus_per_task 8 \
    --slurm_mem 96 \
    --gpu_type v100s \
    --batch_size 256 \
    --accumulate_grad_batches 2 \
    --partition ddp-4way \
    --timeout 2880 \
    --ngpus 1 \
    --comment "cifar10 ms l-ext frozen trip" \
    --group qmae_latent_ext \
    --model qmae-latent-extension \
    --config src/qmae_latent_extension/configuration/train.ini \
    --dataset cifar10 \
    --num_tasks 5 \
    --num_epochs_schedule warmup \
    --max_epochs 600 \
    --min_epochs 300 \
    --igpt_num_epochs_max 20 \
    --igpt_num_epochs_min 20 \
    --igpt_accumulate_grad_batches 1 \
    --igpt_batch_size 64 \
    --max_epochs_lin_eval 100 \
    --igpt_mask_ratio 0.02 \
    --num_random_past_samples 25000 \
    --num_random_past_samples_schedule fixed \
    --cycle_consistency_loss_weight 1 \
    --cycle_consistency_sigma 1 \
    --wandb_dir /scratch/shared/beegfs/dzverev/wandb \
    --current_samples_loss_weight 1 \
    --num_random_future_samples 0 \
    --future_samples_mode noise \
    --num_workers 8 \
    --num_embeddings 256 \
    --num_embeddings_per_step 256 \
    --precision 16-mixed
