sbatch submit.sh python train.py \
    --config src/vq_vae/configuration/train.ini \
    --model vq-vae \
    --wandb_dir /scratch/shared/beegfs/dzverev/wandb \
    --max_epochs 220 \
    --max_epochs_igpt 300 \
    --max_epochs_lin_eval 250 \
    --num_random_future_samples 2000 \
    --num_random_past_samples 7000 \
    --downstream_loss_weight 0 \
    --evaluation_logger wandb \
    --train_logger wandb \
    --group igpt-bootstrap \
    --num_workers 12 \
    --future_samples_mode noiseg


TIMM_FUSED_ATTN=1 sbatch submit.sh python train.py \
    --group mae-rec-to-clf-correlation \
    --dataset cifar100 \
    --num_tasks 20 \
    --max_epochs 1200 \
    --min_epochs 300 \
    --num_epochs_schedule warmup \
    --supervised False \
    --devices 0, \
    --save_model_every 25 \
    --config src/transformer_vq_vae/configuration/train.ini \
    --model transformer-vq-vae \
    --wandb_dir /scratch/shared/beegfs/dzverev/wandb \
    --batch_size 512 \
    --accumulate_grad_batches 1 \
    --num_workers 8 \
    --max_epochs_lin_eval 100 \
    --num_random_past_samples 20000 \
    --num_random_past_samples_schedule schedule \
    --num_random_future_samples 0 \
    --future_samples_mode noise \
    --cycle_consistency_loss_weight_for_past 1 \
    --cycle_consistency_loss_weight_for_current 0 \
    --cycle_consistency_sigma 200 \
    --past_samples_loss_weight 0 \
    --current_samples_loss_weight 1 \
    --future_samples_loss_weight 1 \
    --separate_codebooks true \
    --quantize_top_k 1 \
    --num_embeddings 256 \
    --num_class_embeddings 4 \
    --precision 16-mixed
