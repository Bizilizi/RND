{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.benchmarks import SplitMNIST, SplitCIFAR10\n",
    "import lovely_tensors as lt\n",
    "from torchvision import transforms, datasets\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import shutup;\n",
    "import pytorch_lightning as pl\n",
    "import os \n",
    "\n",
    "\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK']=\"1\"\n",
    "os.environ[\"WANDB_START_METHOD\"] = \"thread\"\n",
    "\n",
    "shutup.please()\n",
    "pl.seed_everything(42)\n",
    "lt.monkey_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "benchmark = SplitCIFAR10(\n",
    "    n_experiences=5,\n",
    "    return_task_id=True,\n",
    "    shuffle=False,\n",
    "    dataset_root='./datasets',\n",
    "    train_transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (1.0, 1.0, 1.0))\n",
    "    ]),\n",
    "    eval_transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (1.0, 1.0, 1.0))\n",
    "    ]),\n",
    ")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root='./datasets',\n",
    "    train=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (1.0, 1.0, 1.0)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root='./datasets',\n",
    "    train=False,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (1.0, 1.0, 1.0)),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import argparse\n",
    "from train_utils import get_device, add_arguments, get_wandb_params\n",
    "from src.vq_vae.init_scrips import get_model\n",
    "from src.utils.train_script import overwrite_config_with_args, parse_arguments\n",
    "from configparser import ConfigParser\n",
    "from src.vq_vae.configuration.config import TrainConfig\n",
    "\n",
    "ini_config = ConfigParser()\n",
    "ini_config.read(\"../src/vq_vae/configuration/train.ini\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Model trainer\")\n",
    "parser = add_arguments(parser)\n",
    "\n",
    "## Read args\n",
    "args = parse_arguments(parser)\n",
    "args.accelerator = \"mps\"\n",
    "args.train_logger = \"tensorboard\"\n",
    "args.evaluation_logger = \"int\"\n",
    "args.max_epochs = 320\n",
    "args.min_epochs = 320\n",
    "args.num_workers = 0\n",
    "args.regularization_dropout = 0.0\n",
    "args.regularization_lambda = 0.0\n",
    "args.learning_rate = 0.001\n",
    "args.batch_size = 256\n",
    "args.best_model_prefix = \"artifacts\"\n",
    "args.num_random_noise = 0\n",
    "args.model = \"vq-vae\"\n",
    "\n",
    "config = TrainConfig.construct_typed_config(ini_config)\n",
    "overwrite_config_with_args(args, config)\n",
    "\n",
    "is_using_wandb = (\n",
    "    config.train_logger == \"wandb\"\n",
    "    or config.evaluation_logger == \"wandb\"\n",
    "    or args.run_id\n",
    ")\n",
    "if is_using_wandb:\n",
    "    wandb_params = get_wandb_params(args, config)\n",
    "\n",
    "    wandb.run.name = args.experiment_name or (\n",
    "        f\"RI-0.\"\n",
    "        f\"RN-{config.num_random_noise}.\"\n",
    "        f\"Dr-{config.regularization_dropout}.\"\n",
    "        f\"Wd-{config.regularization_lambda}.\"\n",
    "    )\n",
    "    wandb_params[\"name\"] = wandb.run.name\n",
    "else:\n",
    "    wandb_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from train_utils import get_loggers\n",
    "from src.avalanche.strategies import NaivePytorchLightning\n",
    "from src.vq_vae.model.vq_vae import VQVae\n",
    "\n",
    "device = get_device(config)\n",
    "vq_vae_model = VQVae(\n",
    "    num_hiddens=config.num_hiddens,\n",
    "    num_residual_layers=config.num_residual_layers,\n",
    "    num_residual_hiddens=config.num_residual_hiddens,\n",
    "    num_embeddings=config.num_embeddings,\n",
    "    embedding_dim=config.embedding_dim,\n",
    "    commitment_cost=config.commitment_cost,\n",
    "    decay=config.decay,\n",
    "    learning_rate=config.learning_rate,\n",
    "    regularization_lambda=config.regularization_lambda,\n",
    "    regularization_dropout=config.regularization_dropout,\n",
    "    data_variance=0.06328692405746414,\n",
    "    use_lpips=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vq_vae.init_scrips import get_callbacks\n",
    "\n",
    "train_experience, test_experience = next(iter(zip(benchmark.train_stream, benchmark.test_stream)))\n",
    "# Test trained model\n",
    "cl_strategy_logger, eval_plugin_loggers = get_loggers(config, vq_vae_model, wandb_params)\n",
    "evaluation_plugin = EvaluationPlugin(\n",
    "    suppress_warnings=True,\n",
    ")\n",
    "\n",
    "cl_strategy = NaivePytorchLightning(\n",
    "    accelerator=config.accelerator,\n",
    "    devices=config.devices,\n",
    "    validate_every_n=config.validate_every_n,\n",
    "    accumulate_grad_batches=config.accumulate_grad_batches,\n",
    "    train_logger=cl_strategy_logger,\n",
    "    initial_resume_from=args.resume_from,\n",
    "    model=vq_vae_model,\n",
    "    device=device,\n",
    "    optimizer=vq_vae_model.configure_optimizers(),\n",
    "    criterion=vq_vae_model.criterion,\n",
    "    train_mb_size=config.batch_size,\n",
    "    train_mb_num_workers=config.num_workers,\n",
    "    train_epochs=config.max_epochs,\n",
    "    eval_mb_size=config.batch_size,\n",
    "    evaluator=evaluation_plugin,\n",
    "    callbacks=get_callbacks(config),\n",
    "    max_epochs=config.max_epochs,\n",
    "    min_epochs=config.min_epochs,\n",
    "    best_model_path_prefix=config.best_model_prefix,\n",
    "    plugins=[],\n",
    ")\n",
    "\n",
    "cl_strategy.train(train_experience, [test_experience])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_experience' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Test trained model\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m test_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mtest_experience\u001B[49m\u001B[38;5;241m.\u001B[39mdataset\n\u001B[1;32m      3\u001B[0m test_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(test_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      5\u001B[0m cl_strategy\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39meval()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'test_experience' is not defined"
     ]
    }
   ],
   "source": [
    "# Test trained model\n",
    "test_dataset = test_experience.dataset\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "cl_strategy.model.eval()\n",
    "losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        x, y, *_ = batch\n",
    "        x, y = x.to(vq_vae_model.device), y.to(vq_vae_model.device)\n",
    "\n",
    "        vq_loss, x_recon, quantized, _, perplexity, logits = vq_vae_model.forward(x)\n",
    "        _, reconstruction_loss, clf_loss, clf_acc, _ = vq_vae_model.criterion(\n",
    "            (vq_loss, x_recon, quantized, x, perplexity, logits), y\n",
    "        )\n",
    "        loss = vq_loss + reconstruction_loss\n",
    "        losses.append(loss)\n",
    "\n",
    "avg_test_loss = torch.tensor(losses).mean()\n",
    "print(avg_test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# View Embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "proj = umap.UMAP(n_neighbors=3,\n",
    "                 min_dist=0.1,\n",
    "                 metric='cosine').fit_transform(vq_vae_model._vq_vae._embedding.weight.data.cpu())\n",
    "plt.scatter(proj[:,0], proj[:,1], alpha=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "5da7ba37bc5bdae9b513af7f05bf2924837c9a2f6b279cd0b39590e16ef5df98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
