[training]
accelerator = gpu
devices = 0,
batch_size = 256
max_epochs = 300
min_epochs = 40
max_epochs_lin_eval = 200
min_epochs_lin_eval = 0
validate_every_n = 2
num_workers = 2
accumulate_grad_batches =
learning_rate = 0.001
dataset_path =
best_model_prefix =

[logging]
evaluation_logger = wandb
train_logger = wandb
logging_path = logs

[model]
num_hiddens = 128
num_residual_layers = 2
num_residual_hiddens = 32
num_embeddings = 512
embedding_dim = 64
commitment_cost = 0.25
decay = 0.99
num_random_noise = 0
regularization_lambda = 0.0
regularization_dropout = 0.0
use_lpips = False