[training]
accelerator = cuda
strategy = auto
devices = auto
batch_size = 128
max_epochs = 500
min_epochs = 500
max_epochs_lin_eval = 100
min_epochs_lin_eval = 0
validate_every_n = 2
num_workers = 8
accumulate_grad_batches = 1
learning_rate = 0.001
weight_decay = 0.05
precision = 32-true
warmup = 200
dataset_path = /scratch/shared/beegfs/dzverev/datasets
best_model_prefix = /scratch/shared/beegfs/dzverev/artifacts
num_tasks = 5
dataset = cifar10
image_size = 32

[logging]
evaluation_logger = wandb
train_logger = wandb
logging_path = logs
checkpoint_path = /scratch/shared/beegfs/dzverev/artifacts

[model]
# model parameters
num_embeddings = 24
num_embeddings_per_step = 5
num_class_embeddings = 2
embedding_dim = 192
decoder_embedding_dim = 192
commitment_cost = 0.25
decay = 0.99
use_lpips = True
encoder_layer = 12
encoder_head = 3
decoder_layer = 4
decoder_head = 3
corruption_rate = 0.75
perplexity_threshold = 150
patch_size = 2
supervised = False
# quantization params
quantize_features = True
quantize_top_k = 1
# loss coefficients
cycle_consistency_loss_weight_for_past = 0
cycle_consistency_loss_weight_for_current = 0
cycle_consistency_sigma = 1
past_samples_loss_weight = 1
current_samples_loss_weight = 1
future_samples_loss_weight = 1
data_variance = 0.06328692405746414

[sampling]
num_random_future_samples = 0
num_random_past_samples = 0
num_random_past_samples_schedule = fixed
future_samples_mode = noise
bootstrapped_dataset_path = /scratch/shared/beegfs/dzverev/artifacts
temperature = 1.7
reuse_igpt = True

[igpt]
mask_ratio = 0.75
num_gpt_layers = 12
igpt_batch_size = 64
igpt_accumulate_grad_batches = 1
igpt_mask_ratio = 0.1
igpt_mask_token_weight = 0.95
igpt_epoch_num = 10