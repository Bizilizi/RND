[training]
accelerator = gpu
devices = auto
batch_size = 256
max_epochs = 300
min_epochs = 40
max_epochs_lin_eval = 200
min_epochs_lin_eval = 0
validate_every_n = 2
num_workers = 4
accumulate_grad_batches = 1
learning_rate = 0.001
dataset_path = /scratch/shared/beegfs/dzverev/datasets
best_model_prefix = /scratch/shared/beegfs/dzverev/artifacts

[logging]
evaluation_logger = wandb
train_logger = wandb
logging_path = logs

[model]
num_hiddens = 128
num_residual_layers = 2
num_residual_hiddens = 32
num_embeddings = 512
embedding_dim = 64
commitment_cost = 0.25
decay = 0.99
num_random_noise = 0
regularization_lambda = 0.0
regularization_dropout = 0.0
use_lpips = False
embeddings_distance = cosine
corruption_rate = 0.75
vq_loss_weight = 1
reconstruction_loss_weight = 1
contrastive_loss_loss_weight = 1
encoder_mlm_loss_loss_weight = 1
decoder_regression_loss_loss_weight = 1