[training]
accelerator = gpu
devices = auto
batch_size = 256
max_epochs = 1500
min_epochs = 500
max_epochs_lin_eval = 300
min_epochs_lin_eval = 0
validate_every_n = 2
num_workers = 8
accumulate_grad_batches = 1
learning_rate = 0.001
weight_decay = 0.05

dataset_path = /scratch/shared/beegfs/dzverev/datasets
best_model_prefix = /scratch/shared/beegfs/dzverev/artifacts

[logging]
evaluation_logger = wandb
train_logger = wandb
logging_path = logs
checkpoint_path = /scratch/shared/beegfs/dzverev/artifacts

[model]
num_hiddens = 128
num_residual_layers = 2
num_residual_hiddens = 32
num_embeddings = 512
embedding_dim = 64
commitment_cost = 0.25
decay = 0.99
regularization_lambda = 0.0
regularization_dropout = 0.0
use_lpips = False
embeddings_distance = cosine
corruption_rate = 0.75
vq_loss_weight = 1
reconstruction_loss_weight = 1
contrastive_loss_loss_weight = 1
encoder_mlm_loss_loss_weight = 1
decoder_regression_loss_loss_weight = 1

[sampling]
num_random_future_samples = 0
num_random_past_samples = 0
future_samples_mode = noise
sampling_temperature = 1.0
bootstrapped_dataset_path = /scratch/shared/beegfs/dzverev/artifacts

[igpt]
mask_ratio = 0.75
num_gpt_layers = 12
igpt_batch_size = 64
igpt_accumulate_grad_batches = 1
igpt_mask_ratio = 0.1
igpt_mask_token_weight = 0.95