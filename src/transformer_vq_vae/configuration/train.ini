[training]
accelerator = cuda
devices = auto
strategy = auto
batch_size = 128
max_epochs = 500
min_epochs = 500
max_epochs_lin_eval = 100
min_epochs_lin_eval = 0
validate_every_n = 2
num_workers = 8
accumulate_grad_batches = 1
learning_rate = 0.001
weight_decay = 0.05
precision = 32-true

dataset_path = /scratch/shared/beegfs/dzverev/datasets
best_model_prefix = /scratch/shared/beegfs/dzverev/artifacts
num_tasks = 5

[logging]
evaluation_logger = wandb
train_logger = wandb
logging_path = logs
checkpoint_path = /scratch/shared/beegfs/dzverev/artifacts

[model]
num_embeddings = 128
num_class_embeddings = 64
embedding_dim = 192
commitment_cost = 0.25
decay = 0.99
use_lpips = True
corruption_rate = 0.75
cycle_consistency_loss_weight_for_past = 0
cycle_consistency_loss_weight_for_current = 0
cycle_consistency_sigma = 1
past_samples_loss_weight = 1
current_samples_loss_weight = 1
future_samples_loss_weight = 1
quantize_features = True
separate_codebooks = True
quantize_top_k = 3
supervised = False

[sampling]
num_random_future_samples = 0
num_random_past_samples = 0
num_random_past_samples_schedule = fixed
future_samples_mode = noise
bootstrapped_dataset_path = /scratch/shared/beegfs/dzverev/artifacts
temperature = 1.23
reuse_igpt = True

[igpt]
mask_ratio = 0.75
num_gpt_layers = 12
igpt_batch_size = 64
igpt_accumulate_grad_batches = 1
igpt_mask_ratio = 0.1
igpt_mask_token_weight = 0.95